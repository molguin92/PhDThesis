\todo[inline]{Still needs some work, don't review yet.}

% \section{Related work}
\phantomsection%

\subsection{Emulation in \glsfmtshort{XR} research}

Only a limited number of works addressing the partial or complete emulation of XR applications for benchmarking and research purposes exist in literature.
One such work is~\cite{williams2013transform}.
In this, the authors introduced a trace-based toolkit for the evaluation of tracking algorithms in \gls{AR} research and development.
The trace consists of geographical coordinates and accelerometer data as well as video frames which can be replayed on a desktop visualization tool or a smartphone application for the evaluation of tracking algorithms in \gls{AR} research and development.\@
\cite{munro2016aaremu} presents a trace-based framework that allows a previously recorded trace of inputs to be replayed to the processing components of a real \gls{AR} application by mocking Android \gls{SDK} \glspl{API}.
This approach allows for benchmarking and evaluation of algorithms and architectures on real data.
A similar approach was taken by~\cite{choi2022emulating}, but the authors instead provided developers of \gls{AR} with a framework that abstracts input into general classes, allowing for device-agnostic processing.
Finally, the authors of~\cite{chetoui2022arbench} employ \gls{AR} emulation to evaluate hardware rather than the software implementation itself.
Their tool, \emph{ARBench}, features emulated \gls{AR} workloads that mimic the load placed on the system by real applications.

While the above approaches to benchmarking and research in \gls{XR} are steps in the right direction, they fail to consider the dynamic nature of \gls{XR}.
The use of rigid traces in trace-based approaches can lead to timing mismatches when replaying at runtime, as the system may respond differently to the same input depending on factors such as system load and network congestion.
Additionally, these approaches do not take into consideration the effects of system responsiveness on human behavior, which can be critical for evaluating both \gls{QoS} and \gls{QoE}.
To address these issues, future approaches will need to adapt trace replay at runtime to match expected behaviors and consider the impact of system responsiveness on user behavior.
By doing so, we can develop more accurate and reliable benchmarks for \gls{XR} applications and improve our understanding of how users interact with these technologies.

\subsection{Edge and Cloud Benchmarking}

This dissertation complements existing work on benchmarking distributed system architectures.
A rich body of literature exists with examples of such approaches for Cloud computing.
For example, the \gls{YCSB}~\cite{cooper2010benchmarking} is a data-intensive benchmark for the profiling of data centers and Cloud services; workload characterization in datacenters leads to a similar benchmark in~\cite{jia2013characterizing}.
In~\cite{turner2012cmart}, the authors introduce \emph{C-MART}, a benchmark for modern web applications running on the Cloud.
The benchmark utilizes modern web technologies, can be deployed automatically at a large scale, and includes generators that mimic remote clients accessing the system over the internet.\@
\cite{malawski2018benchmarking,back2018using} focus instead on serverless computing, using compute-intensive workloads to benchmark major Cloud function providers such as \gls{AWS}, Microsoft Azure, and \gls{GCP}.

As evidenced by the given examples, existing approaches to Cloud benchmarking tend to focus heavily on large-scale data processing and downlink transmission in Cloud scenarios.
This makes them unsuitable for Edge Computing, as they fail to account for important characteristics of this paradigm, such as latency-sensitivity of workloads and uplink-heavy network traffic.

The limitations of previous approaches has in later years led to a number of works that have attempted to tackle the challenge of specifically benchmarking Fog and Edge deployments as well.
Even so, these works have generally leveraged static workloads that fail to account for the intensely dynamic nature of \gls{CPS} deployments.
Examples of approaches which fall into this category include
\begin{inlineenum}
    \item \emph{EdgeBench}~\cite{das2018edgebench}
    \item \emph{IoTBench}~\cite{lee2019iotbench}
    \item \emph{DeFog}~\cite{mcchesney2019defog}
    \item \emph{ComB}~\cite{baurle2022comb}
    \item \emph{OpenRTiST}~\cite{george2020openrtist}
\end{inlineenum}.
\emph{EdgeBench}~\cite{das2018edgebench} corresponds to a benchmarking suite for the Edge which emulates three different workloads (speech-to-text, image recognition, and sensor network application traffic), specifically targeting serverless deployments.\@
\emph{IoTBench}~\cite{lee2019iotbench} is a benchmarking tool for Edge-deployed \gls{IOT} applications.
It includes seven representative workloads, and allows researchers to evaluate the performance of these applications on Edge infrastructure through the analysis of computational demands and resource consumption.\@
\emph{DeFog}~\cite{mcchesney2019defog} corresponds to a benchmarking suite for the comparison of deployments on the Edge, Fog, or Cloud which includes six latency-critical and bandwidth-intensive workloads, including real-time video detection and a text-to-speech application.
\emph{ComB}~\cite{baurle2022comb} is a flexible and extensible application-oriented benchmarking suite for Edge computing, featuring a built-in distributed video-analytics benchmark.
Finally, \emph{OpenRTiST}~\cite{george2020openrtist} is a tool for the benchmarking and performance evaluation of Edge Computing deployments using real-time processing of a live video feed.
Video is streamed from a source Android device to a compute node where neural style-transfer~\cite{gatys2016image} is applied to it.
The transformed video is then fed back to the source to be presented, resulting in a compute-intensive, bandwidth-hungry, and latency-sensitive benchmark.

Furthermore, although some of these approaches address some of the challenges to benchmarking \gls{XR} on the Edge, none of them have tackled the topic in a comprehensive way.\@
\cite{das2018edgebench,mcchesney2019defog,baurle2022comb,george2020openrtist} all target image or video recognition or manipulation in some manner.
However, none of these approaches have considered the interactive component of \gls{MAR}, particularly when it comes to human behavior and its relationship to system responsiveness and performance.

\subsection{Experimental research in \glsfmtshortpl{NCS}}

A significant amount of work has been dedicated to the modeling and performance characterization of \glspl{NCS} due to their potential benefits for industrial and commercial settings~\cite{lu2016real,hespanha2007survey,zhang2013network,zhang2016survey}.
Research in \glspl{NCS} has mostly focused on addressing the challenges imposed by the introduction of best-effort communication networks into traditional control systems.
Network delays (latency), losses, and \emph{jitter} --- referring to the variation in random network delays --- are at the forefront of these challenges.
They have partially been addressed through the clever placement of the controller (e.g.\ by employing Edge Computing)~\cite{sasaki2017layered,sasaki2016vehicle}, as well as through the development of control algorithms capable of accounting for the effects of the network~\cite{zhang2013network}.

Most of the large literature concerning \glspl{NCS}, however, follows a theoretical approach, with only small fraction of it dealing with experimental studies~\cite{zhang2019networked}.
\gls{NCS} studies are complex due to the inter-domain nature of these systems, requiring expertise in the fields of communication networks, computing, and control theory.
However, theoretical approaches can only capture network behaviors at a coarse level, and thus a body of works has emerged in later years involving experimental approaches to \gls{NCS} benchmarking.

Experimental research in \glspl{NCS} involves a diverse range of hardware and software platforms, methodologies, and key performance indicators.
Specifically, we identify three distinct categories of approaches: fully practical, fully simulated, and emulated approaches that combine components of the first two.

In a fully practical approach, the experimental setup uses real systems and hardware to test and validate the performance of the \gls{NCS}.
This approach is straightforward to implement, and can be very useful for evaluating the real-world applicability of the system, but can be costly and time-consuming.
Examples of the application of this approach can be found in studies such as~\cite{drew2005networked,baumann2018evaluating,li2014wireless,cuenca2019periodic}.
These studies implemented wireless and microcontroller-based systems for control and validated them on a physical prototype.

In~\cite{drew2005networked}, the authors present a \gls{NCS} model which considers both random packet delay and loss on both the sensor and actuator sides of the feedback loop.
They validate their design on a physical testbed consisting of inverted pendula controlled from a central computation point over Wi-Fi.
A similar testbed is employed for validation in~\cite{baumann2018evaluating}, in which the authors develop an evaluation methodology for wireless \gls{NCS}.\@
\cite{li2014wireless} implements a wireless microcontroller-based system for vibration control and validates it on a physical prototype comprising a cantilever beam controlled using an \acs{IEEE} 802.15.4 \gls{WPAN} system.\@ \cite{cuenca2019periodic} discusses the design and implementation of a periodic event-triggered sampling and dual-rate control techniques for wireless control, and validate their contributions on a four-rotor autonomous drone platform controlled over WiFi.
\emph{NCSbench}~\cite{zoppi2020ncsbench}, an open-source \gls{NCS} benchmarking platform designed with reproducibility in mind built using the \citetitle{LEGOMindstormsEV3}~\cite{LEGOMindstormsEV3} platform, is another such example.

Completely simulated setups, on the other hand, rely on computer simulations to evaluate the \gls{NCS}'s performance.
This approach is less expensive and faster than the practical approach but may not fully capture the real-world complexities of the system.
Examples of tools and frameworks for the implementation of this approach can be found in works such as~\cite{andersson2005simulation,eyisi2012ncswt}.

A simulated approach is employed in~\cite{du2009novel}, in which the authors develop a novel \emph{Smith} predictor to compensate for varying latencies in wireless \glspl{NCS} and validate it using the \emph{TrueTime}~\cite{henriksson2002truetime} simulator.
In~\cite{chen2015synchronous} the authors validate and evaluate a number of synchronous control strategies for three-motor setups using completely simulated \glspl{NCS} environments.\@
\cite{wu2012application} introduces the concept of \emph{network predictive control} in and uses it to balance a simulated inverted pendulum over a simulated wireless network.\@
\cite{ma2019optimal} proposes an optimal dynamic scheduling strategy that optimizes performance of multi-loop control systems and validate it by simulating a four-loop control system over an \acs{IEEE} 802.15.4 \gls{WPAN}.

Finally, network and hardware-in-the-loop approaches, which aim to strike a balance between realism and efficiency, have also been employed in literature.
Network-in-the-loop refers to experimental setups in which the \emph{network} is instead simulated or emulated.
Such approaches have, for instance, been used to study the effects of the \gls{CSMA/CD} medium access control mechanism on plant stability, as in~\cite{natale2004inverted}.
Hardware-in-the-loop approaches, referring to \gls{NCS} setups in which a real network interacts with a simulated or emulated control system, are particularly prevalent in the field of \emph{smart grid} control.
An example of the application of such an approach can be found in~\cite{wang2020inverter}, in which the authors introduce and validate a novel three-level coordinated control method for photovoltaic inverters.

\medskip

The diversity of experimental \glspl{NCS} research discussed above has led to fragmentation of hardware, software, and methodology, as studies tend to favor approaches popular in their respective communities.
Additionally, studies tend to focus on individual aspects and components of the system, resulting in incomplete knowledge of the overall \glspl{NCS}.
There is a gap in knowledge concerning the reproducibility and comparison of experimental studies on these systems.

To our knowledge, the only published attempt at addressing methodological fragmentation and poor reproducibility of experimental \gls{NCS} research is the LEGO-based approach introduced in~\cite{zoppi2020ncsbench}.
However, this approach has limitations.
It is dependent on specific hardware --- the LEGO Mindstorms set --- which makes its adoption limited due to availability issues, reduces its flexibility, scalability, and limits the scope of experiments able to be conducted via \emph{NCSbench} only to what is possible to implement in LEGO.\@
Scaling up experiments becomes cumbersome and costly, further limiting the practicality of the tool.
Thus, a more general approach that tackles the scalability and reproducibility issues of NCS research is urgently needed.
The development of such an approach would enable researchers to conduct experiments that generate reproducible results consistently and can be compared across different studies while still retaining the computational sophistication required when scaling up experiments.

\subsection{Testbed research in Edge Computing}

The last few years have seen the emergence of a number of small- to mid-scale platforms and testbeds for Edge Computing research.
Several of these are additionally driven by research interests in novel mobile networking technologies and \gls{MEC}, as well as in the virtualization of network functions.
Of particular interest to us are the
\begin{inlineenum}
    \item \acs{GENI}
    \item Chameleon
    \item \acs{COSMOS}
    \item \acs{POWDER}
    \item \acs{ARA}
    \item EdgeNet
%    \item Drexel Grid \gls{SDR}
\end{inlineenum} testbeds.

\gls{GENI} is a testbed deployed across more than fifty sites across the \gls{USA}.
Originally designed for ``the development, deployment, and validation of transformative, at-scale concepts in network science, services, and security''~\cite{berman2014geni}, \gls{GENI} today combines local Edge compute resources with mobile network last-hop connectivity for \gls{MEC} experimentation~\cite{gosain2017geni}.
The wireless-capable sites incorporate single-hop access to compute, storage, and network resource, making \gls{GENI} a compelling Edge Computing platform.

The Chameleon testbed is a research platform originally designed for the study and evaluation of Cloud computing systems~\cite{keahey2020lessons}, but which has recently been expanded to target Edge Computing research through the \acs{CHI}@Edge~\cite{chiatedge} project.
Chameleon is a collaborative project operated by a consortium of research institutions and industry partners, and is designed to provide a flexible and reconfigurable infrastructure for researchers to experiment with and test various Cloud computing technologies, algorithms, and configurations.
The testbed provides a highly reconfigurable array of hardware and software resources, including servers, storage systems, and networking equipment, able to simulate various Cloud and Edge Computing scenarios.

\gls{COSMOS} is a testbed deployed in New York City (State of New York, \gls{USA}), containing over \num{200} rooftop, intermediate, and mobile nodes.
The testbeds main \emph{raison d'être} is \gls{MEC} research, with a strong emphasis on wireless networks.
The system includes \glspl{SDR}, \si{\milli\meter}-wave equipment, optical fibers, Cloud integration, and Edge compute for core network functionality and application data processing~\cite{yu2019cosmos,raychaudhuri2020challenge}.

\gls{POWDER} is described by the authors of~\cite{breen2020powder} as a ``city-scale, remotely accessible, end-to-end software defined platform supporting a broad range of wireless and mobile related research''.
It features roughly \num{15} fixed programmable radio nodes, based on off-the-shelves \glspl{SDR}, distributed across a \SI{15}{\kilo\meter\squared} area in Salt Lake City (State of Utah, \gls{USA}).
These radio nodes are combined with Edge-based compute nodes as well as Cloud resources for full \gls{MEC} experimentation.

The \gls{ARA} platform~\cite{zhang2022ara} is a highly flexible testbed based on the \gls{CHI} software suite originally developed for the aforementioned Chameleon testbed~\cite{keahey2020lessons}.
\gls{ARA} is developed by researchers at Iowa State University in Ames, Iowa (\gls{USA}), and spans a rural area with a diameter of over \SI{60}{\kilo\meter}~\cite{zhang2022ara}.
Its core goal is the study and deployment of advanced Edge Computing and wireless platforms and technologies in real-world agricultural and rural settings, and includes a broad range of wireless technologies deployed through both \glspl{SDR} and programmable \gls{COTS} radios, as well as automated ground vehicles, cameras and sensors.

EdgeNet deviates from the approaches described above and represents a fully software-based, massively distributed public platform for Edge Computing research~\cite{cappos2018edgenet,senel2021edgenet1,senel2021edgenet2}.
It consists of a public, modified and customized Kubernetes~\cite{kubernetes} cluster.
The EdgeNet software is available for anyone to download;
individuals and organizations can quickly deploy self-hosted Edge nodes which are then made available to anyone using the platform.
The end result is a flexible and scalable global Edge Cloud simplifying research and prototype development in Edge and Cloud computing.
A similar, closely-related project is KubeEdge~\cite{xiong2018extend}, a software framework for extending Kubernetes clusters from the Cloud to the Edge.

Finally, a number of other, small-scale approaches also exist.
In~\cite{gedawy2016cumulus}, the authors propose Cumulus, a prototype testbed for Edge compute offloading in \gls{IOT} with the explicit goal of providing a generic and heterogeneous platform for Edge Computing research.
The authors of~\cite{rimal2018experimental} propose an experimental testbed for two-tiered Edge architectures in the context of \gls{FIWI} Edge Computing.
This testbed is then employed to validate wireless access and compute resource scheduling strategies in these systems.
In~\cite{yamanaka2021design}, the authors present an experimental Edge Computing testbed capable of determining the offload location of a workload based on desired latency parameters.
\cite{diao2019scalable} propose yet another Kubernetes-based testbed based for Edge task offloading based on heterogeneous hardware including low-power single-board computers such as Raspberry Pis and Jetson Nanos.\@
\cite{moorthy2022cloudraft} proposes \gls{CLOUDRAFT}, a Cloud-based framework for mobile network experimentation, with a focus on simplifying the management of testbed resources.
The goal of this project is to integrate, coordinate, share, and improve upon existing testbeds through a common interface.

\medskip

The implementation of next-generation wireless systems, Cloud, and Edge computing paradigms requires experimentation to fully understand their implications.
As evidenced above, testbeds have emerged for this purpose.
However, there has been little focus on general-purpose, hardware-agnostic software frameworks for the management and automation of these testbeds.
Most existing works use their own platform-specific solutions, which are not compatible with other testbeds and often do not comply with Cloud and/or Edge-native standards.
\gls{COSMOS} and \gls{POWDER}, which use domain-specific languages and virtualization technology based on \glspl{VM} instead of modern Edge-compatible solutions such as containers or serverless computing frameworks.

From the examples above, \gls{CLOUDRAFT} is the only work known to tackle the challenge of general-purpose, hardware-agnostic software frameworks for the management and automation of testbeds, albeit only to a certain extent.
\gls{CLOUDRAFT} integrates, coordinates, shares, and improves upon existing testbeds, using pre-built VMs with necessary software for experiments.
Although it provides some automation for testbed resource provisioning and experiment execution, its focus is largely on the sharing and partitioning of testbed systems.
Testbeds that are currently working with CloudRAFT include an SDR-based testbed and a ground vehicular robot for mobility-related experimentation.

The lack of a unified approach to testbed management and automation hinders the development and implementation of next-generation wireless systems, Cloud, and Edge computing paradigms.
General-purpose, hardware-agnostic software frameworks such as \gls{CLOUDRAFT} can simplify the management and automation of testbeds, promote compatibility with Cloud- and Edge-native standards, and facilitate collaboration among research groups.
There is still remains much work to be done in this area, particularly with respect to the adoption of Edge-compatible solutions that can enable faster and more reliable experimentation.
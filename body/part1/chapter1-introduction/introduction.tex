The advancement of technology has brought about new computing paradigms and network technologies that have the potential to revolutionize the way we think about and approach computing and communication.
One of the most promising of these is Edge Computing, a distributed computing paradigm which seeks to bring computing closer to the end user, reducing latency (i.e.\ the time between input and response) and increasing reliability.

The current model for distributed computing, known as Cloud Computing, allows users to access shared pools of resources, services, and applications over the internet~\cite{gai2012towards}.
Cloud computing offers many advantages, such as virtually unlimited storage and processing power, by virtue of employing economies of scale in large, geographically distributed datacenters.
However, this design leads to trade-offs in bandwidth and, particularly, latency.
The responsiveness of the Cloud is limited by the speed of light, and thus it can suffer from high latency due to the distance data must travel between the end user and the cloud.
This can cause delays in applications that require quick response times, such as real-time video processing or online gaming.
Additionally, the centralized nature of Cloud datacenters means that it can be vulnerable to outages or other disruptions, which can impact large numbers of users simultaneously.

Edge Computing aims to improve upon the shortcomings of Cloud Computing by moving the processing closer to the end user, at the \emph{edge} of the network.
In this context, the ``edge of the network'' refers to locations which are both geographically and topologically (in the networking sense) close to the user, such as cellular service provider towers/base stations or micro-datacenters located within the same urban area as the user.
By placing computing resources in proximity to where data is generated and/or consumed, and thus reducing the distance that data must travel, Edge Computing can greatly decrease latency and improve the responsiveness of applications.
It also allows for more efficient use of network resources, as data does not need to travel back and forth to a remote, central datacenter.

The advent of 5G~\cite{5Gstandard}, the latest generation of mobile communication networks, offers even greater potential for Edge Computing.
5G offers higher data rates, lower latency, and improved reliability compared to previous generations of mobile networks.
This enables even more powerful and sophisticated applications to be run on the Edge, such as \acsp{CPS} and \acsp{NCS}, as well as \acs{MAR} applications.

\acfp{CPS} and \acfp{NCS} refer to systems where physical processes, such as manufacturing or transportation, are controlled  and monitored by computers, usually over communication networks.
Edge Computing, combined with 5G networking, allows these systems to operate in real-time, enabling near instantaneous response times, more efficient operation, and more precise control.
These characteristics are critical for applications such as \glspl{ITS}, autonomous driving, or remote surgery.

\gls{MAR}, on the other hand, is an application of \gls{AR} that involves overlaying digital information onto the physical world using a mobile device.
This definition has in later years been extended to encompass \gls{AR} deployed on wearable devices (such as Google Glass~\cite{googleglass}) as well.
\gls{MAR} allows users to, for example, view real-time information and details about products while in-store, receive instructions and guidance to perform a complex task, or contextual information while visiting a new city.
By using Edge Computing and 5G, \gls{MAR} can operate with much greater responsiveness and reliability, enabling highly immersive and interactive experiences.

Overall, the combination of Edge Computing and 5G holds great promise for the massive deployment of latency-sensitive applications such as the ones discussed above.
Nonetheless, significant challenges remain in the understanding and scaling of these systems, particularly in regard to their complexity and requirements in the context of low-latency, high-reliability computing.

Latency-sensitive applications, such as \glspl{CPS} and \gls{MAR}, involve multiple processing steps that can influence the latency of the system.
There are many trade-offs involved in optimizing the latency of these applications beyond the question of where the compute process is located. 
These include the choice of wireless system for transmitting application data (e.g.\ 4G \gls{LTE} versus 5G), the protocols used to communicate over the network, and the hardware and operating system used in the backend.
In the case of \gls{MAR}, the sensory inputs need to be pre-processed and compressed on-device, before being transmitted to the compute backend for processing.
The backend algorithms need to be carefully designed and implemented to ensure efficient processing and minimal latency. 
Similarly, in \glspl{CPS}, the communication network must be designed to minimize latency and ensure reliable data transmission between the components of the control system.
Delayed or lost data can result in catastrophic failure of these systems.
Other critical aspects with the potential to impact the latency of latency-sensitive applications on the Edge include congestion on the communication loop and fluctuations of the wireless channel.
These issues can lead to delays in data transmission between devices on the network, leading to higher end-to-end latency.

At the time of the writing of this dissertation, no widely adopted methodology exists for the study of these trade-offs.
We aim to contribute to the field by investigating the applications of such a methodology for the study of latency-sensitive applications deployed on Edge infrastructure and network technologies such as 5G.
The proposed methodology aims to enhance the accuracy and realism of results related to Edge infrastructure and 5G networks, particularly in regard to network performance.
Our results will contribute to the development of new techniques and approaches for improving the performance and reliability of \gls{MEC}.

Our methodology is based on the emulation of target workloads on actual Edge infrastructure.
We replace the client side of the system with a realistic emulation of the desired behaviors, implemented in software deployed on \gls{COTS} general-purpose computing devices.
In our initial implementation, these correspond to low-cost, easily replaceable, and scalable Raspberry Pi 4 Model B \glspl{SBC}.

Emulating the workload component reduces complexity by moving it into the software domain, allowing for easier scaling through the use of cheap, COTS general-purpose hardware such as \glspl{SBC}.
It also preserves the realism of effects stemming from the hardware and network.
In particular, the methodology allows us to capture effects due to network factors such as contention, congestion control, and medium access, which are often of stochastic or chaotic natures and complex to capture in simulations.

The methodology also provides improved repeatability and replicability.
Repeating a study becomes a matter of re-running the workload on the same testbed, and studies can be replicated simply by obtaining the same or equivalent software workload and deploying it on a comparable testbed.
These are complex tasks to accomplish in real-world approaches, particularly when dealing with humans.

Our methodology provides a comprehensive and realistic assessment of the performance of latency-sensitive applications deployed on Edge infrastructure.
The approach provides valuable insights for researchers, system designers, and application developers, and will contribute to the development of new techniques and approaches for improving the performance and reliability of Edge Computing and 5G systems.

% \subsection{Structure of this dissertation}

\medskip

This dissertation is structured into two parts.
\cref{part:summary} presents a summary of the research, introducing the topic, discussing related work, and highlighting key contributions.
Next, in \cref{part:publications} we present the publications that form the core of this thesis.

\cref{part:summary} is itself structured into four main chapters.
\cref{chap:introduction} serves as an introduction to the topic and outlines the key contributions of this work.
It sets the stage for the subsequent chapters by providing a high-level overview of the research problem and its significance, as well as provides background information on the core topics of the thesis.
In \cref{chap:relwork}, we present the scope of the thesis, as well as relevant related work.
The chapter provides a comprehensive review of the existing literature, highlighting the current state-of-the-art and identifying any gaps in knowledge.
We discuss context for our research, and establish the foundation for the contributions presented in the dissertation.
\cref{chap:contributions} is the core of \cref{part:summary} and provides a detailed summary of the contributions made in this dissertation.
We discuss methods, tools, and findings that were developed and obtained throughout the course of our research.
We aim to demonstrate the significance of our work and how it contributes to the field.
Finally, \cref{chap:conclusions} concludes the dissertation and outlines avenues for future research.
We summarize the key findings and contributions of the thesis, and briefly discuss the implications of our findings and contributions for the field.
This chapter also identifies open questions and areas for further research, providing opportunities to build on the work presented in this thesis.
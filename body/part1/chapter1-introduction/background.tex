\section{Background}

\subsection{Edge Computing}
\glsresetall%

\begin{figure}
    \centering
    \includegraphics[height=30em]{figures/edgecomputing}
    \caption{%
        Conceptual design of Edge Computing.
        Compute nodes are placed a at the edge of the network, a few hops away from end users.
        In other words, compute is located between users and the internet and the Cloud.
    }\label{fig:edgecomputing}
\end{figure}


Edge Computing is a novel distributed computing paradigm, emerging from a need to overcome the drawbacks of offloading computation and data to the Cloud.
Cloud computing, the reigning distributed computing model, allows users to access shared pools of resources such as servers, databases, and applications, over the internet~\cite{gai2012towards}.
These pools of resources are managed in a centralized manner by specialized providers, and users and businesses can access them on-demand, without having to invest in and manage infrastructure of their own.
Providers in turn employ economies of scale, providing these services by deploying massive amounts of computing power and storage capacity in specialized locations known as datacenters.
These hardware resources are then further compartmentalized through the use of virtualization technologies such as \glspl{VM} and containers~\cite{gai2012towards}.

Through this design, Cloud computing affords significant advantages to users.
As services are deployed in a centralized manner accessible over the internet, users can interact with their data and applications from anywhere in the world, from any device.
Thanks to economies of scale and virtualization technologies, the Cloud is highly scalable;
services can be scaled simply by spawning more \glspl{VM}.
The specialized nature of Cloud providers, the scale of modern datacenters, and the use of virtualization also make the Cloud highly reliable.
When hardware fails, recovering is simply a matter of migrating the service container or \gls{VM} to an available compute node~\cite{endo2016high}.

However, the Cloud is not suitable for everything, and presents important drawbacks and challenges for latency-sensitive and/or bandwidth-intensive applications.
In order to achieve the necessary economies of scale, Cloud datacenters are designed to serve users distributed across vast geographical areas.
These installations are thus often located ``far'' from potential users;
for instance, at the time of writing, \gls{AWS} processes traffic from all of Scandinavia and the Baltic countries through a single datacenter in Stockholm~\cite{awsregions}.
This leads to prohibitively high latencies for both highly interactive immersive applications such as mobile \gls{XR} and for \glspl{CPS} and \glspl{NCS}~\cite{tolia2006quantifying,lagar2007interactive,satyanarayanan2009case,varghese2016challenges,shi2016promise}.
The former category requires \emph{motion-to-photon} latencies (i.e.\ time between input capture and feedback) below \SI{60}{\milli\second} for interactions to be perceived as fluid and responsive by the user~\cite{chen2017empirical}; the latter can require sub-\SI{10}{\milli\second} latencies, for instance in the case of vehicular safety systems.
Such latencies are unfeasible to consistently achieve with Cloud computing~\cite{dang2021cloudy}.
On the other hand, as smart devices, appliances, and sensors become more and more ubiquitous, the network architectures of modern datacenters face increasing challenges to deal with the massively increasing volume of traffic~\cite{shi2016edge,wang2019towards}.

\medskip
Edge Computing emerges as a potential answer to these challenges~\cite{satyanarayanan2009case,shi2016promise,shi2016edge,varghese2016challenges,satyanarayanan2017emergence,bittmann2017edge,wang2019towards}.
The foundation for this concept was laid by \citeauthor{satyanarayanan2009case}~\cite{satyanarayanan2009case} in\ \citeyear{satyanarayanan2009case}.
To tackle the prohibitively high latencies of Cloud computing, the authors proposed an extension of the existing Cloud computing paradigm with compute nodes at the edge of the Internet.
Instead of offloading computation to a datacenter potentially thousands of kilometers away, in Edge Computing it is offloaded to a compute node at the \emph{edge} of the network close to the user, see \cref{fig:edgecomputing}.

The architecture of Edge Computing offers several advantages, the primary being the significant reduction of latency.
Edge Computing can serve highly latency-sensitive and resource-intensive applications with extremely low latencies while still providing orders of magnitude more computing and energy resources than those present on mobile and wearable devices.
Another important advantage of this design is bandwidth reduction.
Edge application traffic either does not need to traverse the public internet, or it can be aggregated and compressed at the Edge before uplink transmission.

\input{tables/cloud-vs-edge}

\Cref{tab:cloud-vs-edge} shows an overview of the differences between Cloud Computing and three implementations of Edge Computing, \emph{Cloudlet}-based Computing, \gls{MEC}, and \emph{Fog} Computing.
The first of these corresponds to the original Edge Computing concept as introduced in~\citeauthor{satyanarayanan2009case}.
It is based on the idea that Edge compute capabilities will be realized through geographically distributed micro-datacenters known as \emph{cloudlets}.
Cloudlets have been envisioned as featuring most of the key characteristics of Cloud datacenters, such as multi-tenancy, virtualization of computing resources, virtually unrestricted access to energy, as well as limited scalability, all while being one or two hops away from the user.
This design builds on previous work on \emph{cyber foraging}~\cite{noble1997agile,flinn1999energy,satyanarayanan2001pervasive}.
Cyber foraging refers to the extension and amplification of the capabilities of mobile and wearable devices by offloading computation and data manipulation to nearby infrastructure (as opposed to distant infrastructure such as the Cloud).

\glsreset{MEC}%
\glsreset{RAN}%
In later years, an extension to Edge Computing denoted \gls{MEC} has emerged, which pairs Edge Computing with novel mobile networking standards such as 5G~\cite{hassan2019edge,pham2020survey,wan2020efficient}.
A modern mobile network is composed of two main components: the \gls{RAN} and the core network.
The former includes the base stations and other wireless communication equipment to which user's equipment (i.e.\ mobile phones, mobile internet dongles, etc.) connect. 
The \gls{RAN} provides access to the core network, which in turn routes data between \glspl{RAN}, handles billing and authentication, and provides access to external services such as the internet and the Cloud.
\gls{MEC} is defined as an Edge Computing deployment in which Cloud-like compute is deployed within the \gls{RAN} of a mobile network.
Compute power is never more than a single wireless hop away from users application traffic thus never enters the core or the public internet.
This architectural design has the potential to enable unprecedented use-cases requiring stringent, sub-\SI{10}{\milli\second} latency bounds and high bandwidths.
Two examples of such applications, of relevance for the present work, are \acl{WCA}~\cite{ha2014towards,chen2018application,wang2020scaling,chen2017empirical,chen2018application} and \aclp{NCS}~\cite{sasaki2016vehicle,wang2018bandwidth,wan2020efficient}.
These applications had been hitherto inviable to implement due to the limitations of Cloud offloading in mobile and wearable contexts, but have now become a practical reality thanks to \gls{MEC}.

Finally, another variant of Edge Computing called \emph{Fog} computing, introduced by \citeauthor{bonomi2012fog}~\cite{bonomi2012fog} in\ \citeyear{bonomi2012fog}, concerns itself specifically with the distribution of computing power between the Cloud and the edge for the reduction of bandwidth requirements.
By aggregating, transforming, and filtering data at multiple levels in the network, Fog computing aims to reduce the load placed on Cloud datacenters by massive \gls{IOT} sensor networks~\cite{yi2015survey}.
Edge Computing also presents opportunities for increased data security, privacy, and integrity, by keeping it geographically close to its origin and by allowing for anonymization through denaturing and local aggregation before offloading to Cloud services~\cite{satyanarayanan2017emergence}.

\subsection{\glsfmtlong{XR}}\label{background:xr}
\todo[inline]{Needs citations.}
\glsreset{XR}
\glsreset{AR}
\glsreset{VR}
\glsreset{MR}

\gls{XR} is an umbrella term used to refer to all immersive technologies that combine real and virtual environments, including technologies such as \gls{AR}, \gls{VR}, and \gls{MR}.
\emph{Immersiveness} refers to the extent to which a technology can fully engage a user's senses to create the illusion of being in a different environment or situation.
A highly immersive experience can make the user feel like they are physically present in a different world or reality.

\gls{AR} merges digital content with the physical world, enhancing human perception and cognition through the overlay of virtual objects and information on top of the real environment.
This technology can superimpose tooltips, virtual \gls{2D} and \gls{3D} objects, or even full-fledged videos on top of what the user sees.
These overlays are most often presented to the user through wearable or mobile devices, such as smartphones or smart glasses (e.g. Google Glass).
To create an \gls{AR} experience, the device's camera captures the real-world environment, and the image feed is processed in real-time in software.
The digital objects are then superimposed onto the real-world view and displayed on the device's screen, where they can optionally be interacted with by touching the screen or using voice commands.
Other sensors can also be used to track the user's movements and position, and adjust the digital objects accordingly.

\gls{VR}, on the other hand, takes it a step further and involves forgoing the real world, completely immersing the user in a fully virtual environment.
Due to its deeply immersive nature, \gls{VR} requires users to wear specialized equipment such as a \gls{VR} headset.
The headset contains two high-resolution displays (one for each eye) and a set of headphones, and is connected to a computer or a gaming console, which generates the virtual environment in real-time.
The headset tracks the user's head movements and adjusts the virtual displays accordingly to create the illusion of being in a completely immersed in a different world.
Objects in the virtual environment can be interacted with either through the use of special hardware such as instrumented gloves or controllers, or by using sensors which track the user's hands.
Some advanced \gls{VR} platforms even include omnidirectional treadmills which allow the users navigate the virtual world by walking, running, and even jumping. 

Finally, \gls{MR} combines elements from both aforementioned technologies.
Like \gls{VR}, \gls{MR} uses a headset with high-resolution displays and sensors to track the user's movements.
However, \gls{MR} headsets also contain cameras that capture the real-world environment, in order to overlay digital objects onto the real world as in \gls{AR}.
These objects are rendered in real-time and projected onto the real-world view in the user's headset in a seamless and realistic way.
In this way, \gls{MR} creates a hybrid environment where users can interact with both virtual and real-world elements simultaneously, resulting in an illusion of natural interaction between users and digital elements.

\input{tables/extendedreality.tex}

A comparison of select characteristics of \gls{VR}, \gls{AR}, and \gls{MR} is presented in \cref{tab:xrcomparison}.
A key factor in which these applications differ is in their respective \gls{QoS} requirements.
Out of the three, \gls{AR} has the least stringent requirements.
\gls{AR} in particular does not require a high bandwidth, as only individual digital objects are generated.
However, low latencies are still a requirement, in order to ensure that the superimposition of the digital objects on top of the real-word view is accurate and in real-time.
In addition, \gls{AR} requires a high level of accuracy in tracking the user's movements and the position of the device's camera, as inaccuracies can lead to misalignment between the real-world view and the digital objects, leading to degraded \gls{QoE}.

\gls{VR}, on the other hand, requires a high level of \gls{QoS} in terms of both latency and bandwidth to deliver a smooth, highly immersive, and realistic experience.
Moreover, delays or jitter in \gls{VR} experiences can lead to motion sickness in users.
The user's movements and interactions must thus be tracked and rendered accurately in real-time, which requires low latencies between the headset and the computing device.
A high bandwidth is also necessary to transmit high-resolution graphics and audio back to the headset.
Given the similarities between the technologies, these same requirements also apply to \gls{MR}.

\medskip

Due to their ability to create immersive experiences for users and closely mimic and simulate real-life scenarios, \gls{XR} systems have found applications across a wide spectrum of industries and fields.
One of the most common uses of these technologies can be found in the domain of entertainment.
\gls{VR} allows users to become fully immersed in virtual experiences, including games, concerts, movies, making them feel almost physically present without having to leave the comfort of their homes.
Further, \gls{AR} and \gls{MR} allow for the merging of the virtual and real worlds, bringing entertainment to life in users day-to-day surroundings.
The popularity of this approach can be clearly seen in the explosive success of modern mobile \gls{AR} games such \citetitle{pokemongo}~\cite{pokemongo}.

\gls{XR} also presents tremendous opportunities for education and training.
Through the use of these technologies, students can get hands-on yet safe, simulated experience in a multitude of scenarios.
\gls{VR} is leveraged, for instance, in the training of new medical professionals, allowing students to learn anatomy and practice surgical procedures in realistic virtual environments.
On the other hand, \gls{AR} and \gls{MR} are employed in manufacturing, to train and assist technicians in highly specialized assembly lines.
Workers can receive real-time guidance through \gls{AR}-enabled headsets, improving accuracy and safety while controlling machine operations on the factory floor.
\gls{AR} can also provide augmented, step-by-step guidance for maintenance and repair of equipment, augmenting thus the capabilities of workers and the productivity of the workplace as a whole.

\subsubsection{\glsfmtlong{WCA}}\label{sec:background:wca}
\glsreset{WCA}

Of particular interest for this dissertation is a particular subset of \gls{AR} denoted \acf{WCA}, a technology designed to provide real-time support and guidance to individuals performing complex tasks in a plethora of domains.
Designed to be deployed on wearable devices, \gls{WCA} overlays digital information onto the real world and provides context-aware feedback to users, in order to assist and guide them in to efficiently complete a task~\cite{ha2014towards,chen2015early,chen2018application,wang2020scaling}.

Originally motivated by assistive use-cases for individuals with reduced cognition due to illness, injury, or age, \gls{WCA} technology has evolved to include the enhancement of the cognitive abilities of users in areas such as manufacturing, healthcare, and aviation.
Among the primary benefits of this technology is that it can help reduce the cognitive load on individuals performing complex tasks.
By providing real-time guidance and support, it can improve accuracy and speed of task completion, reduce errors and minimize the risk of accidents.
For example, a \gls{WCA} could help a technician performing maintenance on an aircraft by displaying relevant technical information about the aircraft parts and step-by-step instructions on how to perform complex specific repairs.
\gls{WCA} is a promising technology that has the potential to revolutionize the way we perform complex tasks.
As the technology continues to evolve and become more sophisticated, it is expected to have a significant impact on many industries and improve the safety and efficiency of numerous processes.

\medskip

\begin{figure}
    \centering
    \includegraphics[width=.9\textwidth]{figures/wca_state}
    \caption{Mode of operation of step-based \acs{WCA} applications.}\label{fig:wca:operation}
\end{figure}

In this dissertation, we focus specifically on \emph{step-based} \gls{WCA}.
These applications operate in a manner analogous to how \gls{GPS} navigation assistants guide users, illustrated in \cref{fig:wca:operation}.
The progress of the user is seamlessly and continuously monitored by the application, which provides relevant instructions and feedback only when these are needed.
The application follows the progress of the task in real-time by repeatedly sampling the state of the physical system, most commonly through video feeds.
Whenever the assistant detects a change in the context of the user, it provides a new instruction.
The application otherwise remains silent and out-of-the-way; samples with unfinished states or noise are silently discarded.

This mode of operation has novel consequences for the \gls{QoS} requirements of \gls{WCA} applications.
As with general \gls{AR}, \gls{WCA} has relatively lax bandwidth requirements; however, latency requirements in \gls{WCA} can vary along the execution time of the application.
Users of these applications can only notice system responsiveness when they are \emph{expecting} feedback, that is, immediately after the completion of a logical step or instruction.
Latency bounds can thus be very lax immediately after an instruction is generated (as no feedback is expected yet), and on the other hand much more stringent right after the user has completed an instruction.
In turn, this leads to interesting opportunities for resource consumption optimization in \gls{WCA} applications, such as the adaptive sampling scheme proposed in~\cite{wang2019towards}.
The fact that users \emph{consciously} expect near-instantaneous feedback immediately after finishing an instruction also leads to strong and cumulative reactions to delays in the system, as evidenced by our work in \cref{paper:olguinmunoz2021impact}, implying that careful consideration to latencies must be taken when designing these systems.

% As with many other categories of \gls{AR} applications, the computational complexity of the processing of the real-time input data makes it unfeasible to do on wearable devices.
% This context-rich and latency-sensitive nature of \gls{WCA} inputs further makes this category of applications a prime candidate for offloading to the Edge.

\subsubsection{Emulation and simulation for \acs{XR}}
\todo[inline]{
    Section 1.1.2: This subsection is not very informative so far. Maybe first discuss extended reality, and then present a subsection on performance evaluation of systems, where simulation is a subsection, and experimentation is another subsection (and mathematical analysis is a third one). Focus on the usage of these concepts in the context of networked systems. Why do we need tools/approaches for performance evaluation, what are the advantages and disadvantages of different approaches, and also what is the historical context. You also need to have references. Finally, tables and figures are always welcome...
}


The terms \emph{emulation} and \emph{simulation} are often used interchangeably.
However, although both terms refer to methods of imitating and modeling systems, they are distinct in their goals, methods, and applications.

Emulation refers to the process of reproducing or mimicking the observed behavior of a system through another system, 
with the ultimate goal being the recreation of behavior that matches the original system as close as possible.
Emulations run in \emph{real time}, and can thus interact with real environments;
in turn, this means that emulations can be used to replace subcomponents of larger systems for research or benchmarking.
This method does not aim to model the original system, nor aim to understand its underlying behavior.

Simulation, on the other hand, is the process of modeling a system to study its behavior through the creation of a mathematical or logical model of the system.
The model can then be adjusted to simulate and better represent the original system in different scenarios and conditions.
Simulations run in entirely virtualized environments, and are not beholden to any real-time constraint.

Finally, these approaches differ as well in the level of accuracy each of them affords.
While simulation only attempts to approximate the behavior of the system in a given context, emulation aims to represent as close to a perfect copy of the observed behavior of the system.
The former is therefore employed when a general understanding of the system is sufficient, while the latter is used when precision is required.
However, thanks to its real time constraint and high degree of realism, emulation is typically more resource-intensive than simulation.
This means that emulation requires more processing power, memory, and storage space than simulation.

\subsection{\glsfmtlongpl{NCS}}\label{background:ncs}
\glsreset{NCS}

% \todo[inline]{
%     - Section 1.1.4: Goes again in the right direction, but details are missing/off. First of all, NCS is really a research direction, not necessarily a system type (every control system is more or less networked, but networking effects have not been always captured in the theoretical analysis/design of the controller). Second, maybe you stress more the working principle in terms workloads and requirements. In a sense, you can work out a pattern how you structure the discussion in Section 1.1.3, and reuse that pattern to a large extent in 1.1.4, that would be neat (both are closed-loop applications with a lot of similar, but also some different properties and requirements). 
% }

A control system is a collection of devices or subsystems that work together to achieve a desired behavior or outcome of a physical system.
These systems are composed of
\begin{inlineenum}
    \item a \emph{plant}
    \item \emph{sensors}
    \item \emph{actuators}
    \item a \emph{controller}
\end{inlineenum}.
Plant refers to the physical system being controlled.
Sensors sample and encode the state of the plant and transmit it to the controller.
The controller processes these inputs, and generates control signals to be sent to the actuators, which modify the plant's behavior.
The overall goal of a control system is to regulate a process or a physical system to meet desired specifications or objectives, such as maintaining a constant temperature, tracking a set trajectory, or regulating the speed of a machine.

\gls{NCS} research is a research direction in the field of automatic control that focuses on the study and development of control systems that formally account for the effects of incorporating communication networks for data exchange between different components of the system.
Every modern control system is, to some extent, networked in the sense that a communication medium is used to transmit information between components.
The field of \gls{NCS} research formalizes and extends this characteristic by integrating traditional control systems with general-purpose \acs{TCP}/\acs{IP} networks.
\glspl{NCS}~\cite{gupta2010networked} aim to enhance and extend the capabilities of traditional control systems by decoupling the controller from the plant, sensors, and actuators (see \cref{fig:csvsncs}).
This allows for remote monitoring and control, which enhances the reliability and safety of the control system
This is particularly useful in applications where real-time monitoring and control are critical, such as in industrial automation and in transportation systems.
\glspl{NCS} also enable the integration of multiple control systems, as well as centralized control of multiple plants by a single controller.
This leads to enhanced scalability and flexibility, and allows improved coordination and collaboration among systems.
Furthermore, \glspl{NCS} provide greater access to information, as they enable the exchange of data and information between control components and systems, leading to more informed decision-making and improved performance.

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/control_system}
        \caption{%
            Traditional control system.
        }
    \end{subfigure}%
    \hfill%
    \begin{subfigure}[t]{0.50\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figs/networked_control_system}
        \caption{%
            \acl{NCS}.
        }
    \end{subfigure}
    \caption{%
        Comparison between traditional control systems and \aclp{NCS}.
        In an \aclp{NCS}, controller and plant are physically separated and both actuation commands and sensor inputs are shared through a general purpose \acs{TCP}/\acs{IP} network.
    }\label{fig:csvsncs}
\end{figure}

\input{tables/ncs-examples}

\glspl{NCS} have become increasingly popular in recent years due to advances in communication and control technologies, as well as the increasing demand for real-time, remote, and distributed control.
They have found use in varied applications, such as industrial automation, transportation systems, and smart buildings, and some have attempted to leverage the Cloud for centralized, distributed control in what has been called \emph{Cloud control systems}~\cite{xia2015cloud}
However, \glspl{NCS} can have timing requirements for communication that conventional Cloud paradigms simply cannot meet~\cite{wan2020efficient}.
Depending on the system being controlled, control systems can have sampling rates ranging from sub-\SI{1}{\hertz} to several \unit{\kilo\hertz}.
Reliability is another concern, as many of the core use cases for \gls{NCS} involve application domains in which failure has the potential to cause bodily harm to humans or significant financial losses.
\cref{tab:controlsystemqos} lists some example use cases for \gls{NCS} and their \gls{QoS} requirements.
These stringent requirements have led their adoption to be limited mostly to industrial environments.
This is about to change, however, as with the advent of \gls{MEC}, consumer-grade \glspl{NCS} will be made possible.
This technology will likely become the backbone of consumer-grade, widely-deployed \glspl{NCS}, enabling real-time capabilities through extremely low end-to-end latencies together with context- and locality-awareness.

\section{Future Work (ExPECA 2.0)}

\begin{description}[wide,style=nextline]
    \item[Client-Server Architecture and Microservices]
    Our current design follows a monolithic approach, in which ``layers'' of software build upon each other to reach a desired state.
    This approach is easy to understand and somewhat straightforward to implement, and works well for end-to-end experimentation.
    However, it causes a couple of problems, for example:

    \begin{enumerate}
        \item Components in the implementation are quite tightly coupled, so adding new functionality or layers in many cases implies rewritting large chunks of code.
        This also requires quite a lot of synchronization between people working on different parts of the code.
        \item It makes experimentation synchronous, and when running things over \gls{SSH}, it requires the use of tools such as \texttt{screen} or \texttt{tmux} to be able to run long experiments without having to keep the connection open at all times.
        \item We currently have no way of ensuring mutual exclusion of multiple instances of Ainur on the testbed, and implementing such functionality would probably require some sort of centralized instance tracking mechanism.
    \end{enumerate}

    I (Manuel) suggest then that for ExPECA 2.0 we switch to a distributed client-server architecture, using a microservice for each core functionality.
    In other words, we would have several smaller server applications running on potentially different hosts on the network, each handling a specific part of the functionality we need.
    For instance, we could have a ``Layer 3'' service which handles setting up \gls{IP}-layer connectivity, and another ``SDR'' service which manages the \glspl{SDR}.
    Services could communicate with each other using simple HTTP \glspl{API} for easy debugging, and we would have an ``Ainur'' client which orchestrates the services into an experimental run.

    This has several advantages:

    \begin{enumerate}
        \item It decouples the implementation details for each service.
        As long as the \gls{API} is well-defined, it doesn't matter how the service is implemented - we could even use different languages for different functionalities.
        \item Adding new functionality becomes simply a matter of writing a new microservice and plugging it into the existing setup.
        \item Experimentation can be done asynchronously through an ``experiment scheduler'' service which queues, runs, and then collects results from experiments.
        This service would also then ensure mutual exclusion of testbed resources.
        \item Things could be parallelized.
        Our current design is sequential, even when it doesn't need to be.
        Things that do not depend on each other could be executed in parallel (e.g. cloud and local configuration), cutting down on experiment time overhead.
        \item It would fit squarely into the design of both Docker and Kubernetes, both of which are distributed client-service architectures.
    \end{enumerate}

    \item[OpenStack/Canonical MaaS]
    
    We should really consider switching to a mature framework for bare-metal provisioning instead of doing it manually with Ansible.
    Potential options are OpenStack Ironic and Canonical Metal-as-a-Service.

    \item[Raspberry Pi SSDs]
        
    Currently our Raspberry Pis boot Ubuntu 20.04 LTS from SD cards; however this is very taxing on these storage devices, and their estimated lifetime doing this is less than a year.
    We should consider switching to the slightly more expensive option of booting the Raspberry Pis from solid-state drives.
    An alternative which I am studying would be to boot all the Raspberry Pis from a SINGLE shared storage device using iSCI and network booting (PXE). 

\end{description}

% - microservices
% - openstack
% - raspberry pi rack, shared storage?

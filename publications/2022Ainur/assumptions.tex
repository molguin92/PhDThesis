\subsection{Underlying Assumptions}\label{sec:assumptions}



In order to be able to provide as general a platform as possible for containerized workloads, and target a broad spectrum of applications and network configuration, the design of Ainur needs to make a number of assumptions about the underlying testbed platform.
In this section we will detail these assumptions, and try to provide some general context and reasoning behind each of them. 


\subsubsection{Time synchronization}
Hosts on the testbed synchronize their clocks to a central \gls{NTP} server.
This is however only required for consistent time measurements \emph{across} devices; measurements with respect to the clock of a single device (e.g.\ \gls{RTT} measurements) do not require synchronization.

\subsubsection{Network architecture}
Ainur makes strong assumptions about the layout of the testbed network.
A general overview of these can be seen in \cref{fig:network}.

In particular, the network follows a split architecture composed of two sub-networks: a control network for control plane data, and a workload data network for workload payload traffic.
It is designed like this to
\begin{enumerate*}[itemjoin={{, }}, itemjoin*={{, and }}]
    \item be able to dynamically reconfigure, segment, and route the workload data network for different experiments
    \item ensure consistent and accurate network traffic measurements (\glspl{RTT}, bandwidth, etc.) on the workload data network, uncontaminated by control traffic.
\end{enumerate*}

\begin{description}[style=unboxed]
    \item[Control network]
    
    The control network, whose links are represented in orange in \cref{fig:network}, carries control plane data between the control host and the rest of the hosts on the testbed.
    It also provides internet access through a gateway router, and control plane access to the cloud instances through the control \gls{VPN} gateway (more details on this below).
    
    \item[Workload data network]
        
    This network carries data generated by the workloads deployed on the testbed, and is represented by blue links in \cref{fig:network}
    It also provides access to the workload data \gls{VPN} link to cloud instances through the workload data gateway.
    Although the physical links between devices are always present, interfaces on this network by default have no \gls{IP} addresses --- these are managed dynamically by Ainur.

    \item[Managed switch]
    
    Devices on the workload data network are interconnected through a managed switch.
    This switch is used by Ainur to change the virtual topology of the workload data network at runtime using \glspl{VLAN}, for instance separating the network into front-end and back-end networks.

    In order to manage it through software, the managed switch is also connected to the control network through a single link.
    The associated interface is on a separate, exclusive \gls{VLAN}, and no data from the workload data network is allowed to enter the control network.

    \item[\gls{VPN} gateways]
    
    Two \gls{VPN} gateways are present on the testbed; one for control traffic, and one for workload data traffic to \gls{AWS} cloud instances.
    More details on these gateways are given in \cref{sec:layer3}.
    
\end{description}

\subsubsection{Separate control host}

Ainur is executed from a host separate from hosts where the workload is actually run.
This is to avoid issues that could arise from having the control plane residing on the same physical device as the workload data plane, such as traffic cross-contamination, routing issues, etc.

\subsubsection{Workload host network capabilities}

Hosts on which workloads are to be deployed must at the very least have \emph{two} network interfaces. 
One interface is connected to the control network for remote management, and the other to the workload data network, used for workload traffic.

Workload nodes may additionally have WiFi interfaces, but that is not a requirement.

\subsubsection{Software requirements}

Hosts controlled by Ainur are assumed to have a certain set of software packages installed on them.
Most of the below assumptions can be met simply by having hosts run a modern version of Ubuntu (18.04 or later, ideally 20.04).

\begin{description}[style=unboxed]
    % \item[\gls{APT}]

    % Hosts use \gls{APT} to manage installed packages.

    \item[Netplan]
    
    Hosts use Netplan\footnote{\url{https://netplan.io/}} to manage network connections and interfaces.
    
    \item[systemd]
    
    Hosts use systemd\footnote{\url{https://www.freedesktop.org/wiki/Software/systemd/}} to manage system configurations and services.
    
    \item[Docker]
    
    Docker\footnote{\url{https://docs.docker.com/}} is installed on the hosts.
    It is configured to allow regular users to run the Docker client and the server is set up to listen on unencrypted \gls{TCP} port \num{2375} on all interfaces, in addition to the default local UNIX socket.
\end{description}

\subsubsection{Authentication}
As Ainur interacts with hosts remotely, some assumptions are made about the access to these hosts.

\begin{description}[style=unboxed]
    \item[SSH access] 
    
    The \gls{SSH} client on the control host and the \gls{SSH} servers on the controlled hosts (i.e.\ workload hosts, \gls{VPN} gateways) have been configured to allow password-less, public-key based authentication.

    \item[\texttt{sudo} access]
    
    Some of the actions executed on the remote hosts by Ainur require elevated privileges.
    The \texttt{sudo} command on the remote hosts is thus configured to allow password-less privilege escalation.

\end{description} 

\subsubsection{Fluentbit logging}

A Fluentd\footnote{\url{https://www.fluentd.org/}} or Fluentbit\footnote{\url{https://fluentbit.io/}`'} server is running on a host in the control network.
At runtime, this server receives data streams of logs from all services deployed on the workload data network and collects them into log files.

\subsubsection{Cloud integration}

Cloud integration in Ainur is done in \gls{AWS}\footnote{\url{https://aws.amazon.com/}} and the associated \texttt{boto3}\footnote{\url{https://aws.amazon.com/sdk-for-python/}} Python library.

\begin{description}[style=unboxed]
    \item[Authentication to \gls{AWS}]
    
    Credentials to \gls{AWS} exist in the execution environment of Ainur on the control hosts.
    Specifically, these are credentials that allow Ainur to fully manage:
    \begin{itemize}
        \item \gls{EC2} instances, to create and terminate required compute nodes on-the-fly;
        \item security groups, to allow remote access over \gls{SSH} and \gls{VPN} to the compute nodes;
        \item and private access keys, to securely log in to the nodes over \gls{SSH}.
    \end{itemize} 
    
    \item[\glspl{AMI}]
    
    \gls{EC2} instances deployed by Ainur are expected to be based on a Linux \gls{AMI} equipped with the similar software as the local hosts.
    Specifically:

    \begin{itemize}
        \item Docker is installed and configured for regular users and over-the-network access on \gls{TCP} port \num{2375}.
        \item Netplan and systemd are present and in use.
        \item VPNCloud\footnote{\url{https://vpncloud.ddswd.de/}} is installed (see below).
    \end{itemize}
    
    \item[\gls{VPN} gateways]
    
    In order to transparently integrate remote compute instances with the local testbed resources, Ainur uses VPNCLoud to configure tunnels to the cloud for control and workload data traffic.
    This allows the control host and hosts on the workload data network to treat cloud instances as if they resided locally on the control and workload data networks.

    \begin{description}[style=unboxed]
        \item[One gateway per network]
            
        Two gateways are present on the testbed networks, as illustrated in \cref{fig:network}; one for the control traffic \gls{VPN} link, and another for the workload data traffic \gls{VPN} link.

        \item[Public \gls{IP} address]
        
        Both gateways are reachable on the same public \gls{IP} address of the gateway router.
        The gateway router listens on two different \gls{UDP} ports and forwards data to the corresponding \gls{VPN} gateway.

        \item[Pre-shared keys]
        
        Both links share the same VPNCloud authentication parameters, in particular the pre-shared key used to initiate connections.
        
        \item[Routes] 
        
        Static routes to the control \gls{VPN} link through the corresponding gateway are configured on all devices attached to the control network.

    \end{description} 
\end{description}  